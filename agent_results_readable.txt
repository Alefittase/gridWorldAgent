==================== Run 1 ====================
MDP: Deterministic Mode: value iteration, Gamma: 0.6
Iterations: 7, Eval iterations: 0, Runtime: 0.000187 s
Policy Grid:
R R D X D
D X R R D
R D X R D
X R R R U
R U X U U
Value Table Grid:
 -1.9  -1.5  -0.9  NaN   2.0
 -1.5  NaN   0.2   2.0   5.0
 -0.9   0.2  NaN   5.0  10.0
 NaN   2.0   5.0  10.0   0.0
 -0.9   0.2  NaN   5.0  10.0
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4)]

==================== Run 2 ====================
MDP: Deterministic Mode: policy iteration, Gamma: 0.6
Iterations: 3, Eval iterations: 7, Runtime: 0.000416 s
Policy Grid:
R R D X D
D X R R D
R D X R D
X R R R U
R U X U U
Value Table Grid:
 -1.9  -1.5  -0.9  NaN   2.0
 -1.5  NaN   0.2   2.0   5.0
 -0.9   0.2  NaN   5.0  10.0
 NaN   2.0   5.0  10.0   0.0
 -0.9   0.2  NaN   5.0  10.0
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4)]

==================== Run 3 ====================
MDP: Deterministic Mode: value iteration, Gamma: 0.1
Iterations: 5, Eval iterations: 0, Runtime: 0.000144 s
Policy Grid:
R R D X D
D X R R D
R D X R D
X R R R U
R U X U U
Value Table Grid:
 -1.1  -1.1  -1.1  NaN  -1.0
 -1.1  NaN  -1.1  -1.0   0.0
 -1.1  -1.1  NaN   0.0  10.0
 NaN  -1.0   0.0  10.0   0.0
 -1.1  -1.1  NaN   0.0  10.0
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4)]

==================== Run 4 ====================
MDP: Deterministic Mode: policy iteration, Gamma: 0.1
Iterations: 3, Eval iterations: 5, Runtime: 0.000205 s
Policy Grid:
R R D X D
D X R R D
R D X R D
X R R R U
R U X U U
Value Table Grid:
 -1.1  -1.1  -1.1  NaN  -1.0
 -1.1  NaN  -1.1  -1.0   0.0
 -1.1  -1.1  NaN   0.0  10.0
 NaN  -1.0   0.0  10.0   0.0
 -1.1  -1.1  NaN   0.0  10.0
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4)]

==================== Run 5 ====================
MDP: Deterministic Mode: value iteration, Gamma: 0.9
Iterations: 7, Eval iterations: 0, Runtime: 0.000171 s
Policy Grid:
R R D X D
D X R R D
R D X R D
X R R R U
R U X U U
Value Table Grid:
  0.6   1.8   3.1  NaN   6.2
  1.8  NaN   4.6   6.2   8.0
  3.1   4.6  NaN   8.0  10.0
 NaN   6.2   8.0  10.0   0.0
  3.1   4.6  NaN   8.0  10.0
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4)]

==================== Run 6 ====================
MDP: Deterministic Mode: policy iteration, Gamma: 0.9
Iterations: 3, Eval iterations: 7, Runtime: 0.001396 s
Policy Grid:
R R D X D
D X R R D
R D X R D
X R R R U
R U X U U
Value Table Grid:
  0.6   1.8   3.1  NaN   6.2
  1.8  NaN   4.6   6.2   8.0
  3.1   4.6  NaN   8.0  10.0
 NaN   6.2   8.0  10.0   0.0
  3.1   4.6  NaN   8.0  10.0
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4)]

==================== Run 7 ====================
MDP: Stochastic Mode: value iteration, Gamma: 0.6
Iterations: 15, Eval iterations: 0, Runtime: 0.001060 s
Policy Grid:
R R D X D
D X R D D
R D X D D
X R R R U
R U X U U
Value Table Grid:
 -4.3  -4.0  -3.6  NaN  -1.9
 -4.1  NaN  -2.6  -0.7   1.4
 -3.7  -2.9  NaN   2.0   6.9
 NaN  -1.3   1.2   7.0   0.0
 -4.2  -3.0  NaN   1.8   6.8
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 3), (3, 4)]

==================== Run 8 ====================
MDP: Stochastic Mode: policy iteration, Gamma: 0.6
Iterations: 2, Eval iterations: 15, Runtime: 0.000979 s
Policy Grid:
R R D X D
D X R D D
R D X D D
X R R R U
R U X U U
Value Table Grid:
 -4.3  -4.0  -3.6  NaN  -1.9
 -4.1  NaN  -2.6  -0.7   1.4
 -3.7  -2.9  NaN   2.0   6.9
 NaN  -1.3   1.2   7.0   0.0
 -4.2  -3.0  NaN   1.8   6.8
Path: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 3), (3, 4)]
